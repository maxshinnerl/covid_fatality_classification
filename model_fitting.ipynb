{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from   sklearn.compose         import *\n",
    "from   sklearn.experimental    import enable_iterative_imputer\n",
    "from   sklearn.impute          import *\n",
    "from   sklearn.linear_model    import LinearRegression \n",
    "from   sklearn.linear_model    import LogisticRegression, RidgeClassifier\n",
    "from   sklearn.metrics         import mean_absolute_error\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.pipeline        import Pipeline\n",
    "from   sklearn.preprocessing   import *\n",
    "from   sklearn.metrics         import balanced_accuracy_score\n",
    "from   sklearn.inspection      import permutation_importance\n",
    "from   sklearn.decomposition   import PCA\n",
    "from   sklearn.dummy           import DummyClassifier\n",
    "from   sklearn.ensemble        import RandomForestClassifier\n",
    "from   sklearn.model_selection import RandomizedSearchCV\n",
    "from   sklearn.base            import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/sub_COVID-19_Case_Surveillance_Public_Use_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows with missing or unknown value in target (death_yn)\n",
    "Assumption of Supervised Machine Learning is that each instance has a label.  We will discard instances with missing/unknown targets before beginning\n",
    "(Target transformations are ok to do outside of pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_target = df[(df['death_yn'] != 'Unknown') & (df['death_yn'] != 'Missing')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate target from rest of DataFrame\n",
    "Split into Train, Validation, and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df_clean_target.drop('death_yn', axis=1)\n",
    "df_y = pd.DataFrame(df_clean_target['death_yn'])\n",
    "X = df_X.to_numpy()\n",
    "y = df_y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pre, X_test, y_train_pre, y_test = train_test_split(X, y, train_size=0.8)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train_pre, y_train_pre, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Preprocessing Pipeline\n",
    "Our dataset has lots of missing values, some of which have different indicators.  i.e. some are np.nan, others are strings \"Missing\" or \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get categorical columns\n",
    "categorical_columns = (df_X.dtypes == object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous variable preprocessing pipeline\n",
    "con_pipe = Pipeline([('imputer', SimpleImputer(missing_values=np.nan, strategy='median', add_indicator=True)),\n",
    "                     ('scaler', StandardScaler())\n",
    "                    ])\n",
    "\n",
    "# categorical variable preprocessing pipeline\n",
    "cat_pipe = Pipeline([('imputer_nan', SimpleImputer(missing_values=np.nan, strategy='most_frequent', add_indicator=True)),\n",
    "                     ('imputer_missing', SimpleImputer(missing_values='Missing', strategy='most_frequent', add_indicator=True)),\n",
    "                     ('imputer_unknown', SimpleImputer(missing_values='Unknown', strategy='most_frequent', add_indicator=True)),\n",
    "                     ('ohe'    , OneHotEncoder(handle_unknown='ignore'))\n",
    "                    ])\n",
    "\n",
    "# combine preprocessing together\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, categorical_columns),\n",
    "                                   ('continuous' , con_pipe, ~categorical_columns)\n",
    "                                  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple attempt to get baseline BA score\n",
    "Just getting a starting Balanced Accuracy score so that I can tell if future adjustments are making tangible improvements or not.  Fitting a Logistic Regression model with no hyperparameters (aside from the solver, as the default threw errors), we get a 'starting' score of roughly 0.66.\n",
    "\n",
    "I chose *Balanced Accuracy score* here as this is a binary classification problem with a significant imbalance in the target column, both of which are problems that BA socre handles well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('prep' , preprocessing),\n",
    "                 ('lg' , LogisticRegression(solver='liblinear'))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.664964360904198"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train.ravel())\n",
    "y_pred = pipe.predict(X_validate)\n",
    "balanced_accuracy_score(y_validate.ravel(), y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning and Cross Validation\n",
    "Printing available hyperparameters for tuning with Cross Validation.  Good for seeing which type each hyperparameters accepts, as well as just noting all available hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.0,\n",
       " 'class_weight': None,\n",
       " 'copy_X': True,\n",
       " 'fit_intercept': True,\n",
       " 'max_iter': None,\n",
       " 'normalize': False,\n",
       " 'random_state': None,\n",
       " 'solver': 'auto',\n",
       " 'tol': 0.001}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RidgeClassifier().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct dummy Pipeline\n",
    "In cross validation, the Dummy Estimator will be replaced with the specified algorithms in the search space.  The preprocessing pipeline is built above, and is applied first (mostly just handling missing values here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEstimator(BaseEstimator):\n",
    "    \"Pass through class, methods are present but do nothing.\"\n",
    "    def fit(self): pass\n",
    "    def score(self): pass\n",
    "\n",
    "pipe = Pipeline([\n",
    "                 ('prep', preprocessing),\n",
    "                 ('clf', DummyEstimator())\n",
    "                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Search Space for Hyperparameter tuning\n",
    "Cross Validation will try different each estimator algorithm with different subsets of the specified hyperparameter values specified in the search space.  Here I just give the CV a fairly wide breadth of options to choose from just to ensure we get as close to the best model possible as we can.\n",
    "\n",
    "Values for C and penalty were given to Logistic Regression to see if forms of regularization would help.\n",
    "Ridge classifier's max_iter was also given a wide range starting at 1 to see if early stopping would prove beneficial.\n",
    "Random forest was given a range for max_depth of its trees, max_features per split, bootstrapping(T/F), criterion, and n_estimators, the best combination of which would be chosen by the cross validation\n",
    "\n",
    "The most notable feature in the search space here is 'class_weight', which is available for all algorithms.  I've set the cross validation to choos between None and 'balanced' for this, although I would be very surprised if None was chosen over 'balanced', considering the target column has a ton of 'no' values and very few 'yes' values (target imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [\n",
    "                 # LogisticRegression\n",
    "                 {'clf' : [LogisticRegression(solver='liblinear')],\n",
    "                  'clf__penalty': ['l1', 'l2'],\n",
    "                  'clf__C' : np.logspace(0, 4, 10),\n",
    "                  'clf__class_weight' : [None, 'balanced']\n",
    "                 },\n",
    "    \n",
    "                 # RidgeClassifier\n",
    "                 {'clf' : [RidgeClassifier(solver='auto')],\n",
    "                  'clf__tol' : [0.001, 0.01, 0.1],\n",
    "                  'clf__max_iter' : [None, 1, 10, 100, 1000],\n",
    "                  'clf__class_weight' : [None, 'balanced'],\n",
    "                  'clf__normalize' : [False, True]\n",
    "                 },\n",
    "                 \n",
    "                 # RandomForest\n",
    "                 {'clf' : [RandomForestClassifier(n_jobs=-1)],\n",
    "                  'clf__criterion' : ['gini', 'entropy'],\n",
    "                  'clf__class_weight' : [None, 'balanced'],\n",
    "                  'clf__max_depth' : list(range(2,11)),\n",
    "                  'clf__max_features' : ['auto', 'log2', 'sqrt'],\n",
    "                  'clf__n_estimators' : list(range(50, 250, 50)),\n",
    "                  'clf__bootstrap' : [True, False]\n",
    "                 }\n",
    "    \n",
    "    \n",
    "               ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Random CV\n",
    "RandomCV is similar to the standard GridSearchCV.  RandomCV is significantly faster and will still generally yield an extremely good set of hyperparamters as its result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_algos_rand = RandomizedSearchCV(estimator=pipe,\n",
    "                                    param_distributions=search_space,\n",
    "                                    scoring='balanced_accuracy',\n",
    "                                    n_iter=100,\n",
    "                                    cv=5,\n",
    "                                    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and get best model\n",
    "#### NOTE: This takes a while to run.  Feel free to skip this step in your execution, as it is not necessary to run the remainder of code.\n",
    "#### Takes significantly longer on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('prep',\n",
       "                 ColumnTransformer(transformers=[('categorical',\n",
       "                                                  Pipeline(steps=[('imputer_nan',\n",
       "                                                                   SimpleImputer(add_indicator=True,\n",
       "                                                                                 strategy='most_frequent')),\n",
       "                                                                  ('imputer_missing',\n",
       "                                                                   SimpleImputer(add_indicator=True,\n",
       "                                                                                 missing_values='Missing',\n",
       "                                                                                 strategy='most_frequent')),\n",
       "                                                                  ('imputer_unknown',\n",
       "                                                                   SimpleImputer(add_indicator=True,\n",
       "                                                                                 missing_values='Unknown',\n",
       "                                                                                 strateg...\n",
       "                                                  Unnamed: 0                  True\n",
       "cdc_case_earliest_dt       False\n",
       "cdc_report_dt              False\n",
       "pos_spec_dt                False\n",
       "onset_dt                   False\n",
       "current_status             False\n",
       "sex                        False\n",
       "age_group                  False\n",
       "race_ethnicity_combined    False\n",
       "hosp_yn                    False\n",
       "icu_yn                     False\n",
       "medcond_yn                 False\n",
       "dtype: bool)])),\n",
       "                ('clf',\n",
       "                 RandomForestClassifier(bootstrap=False,\n",
       "                                        class_weight='balanced',\n",
       "                                        criterion='entropy', max_depth=9,\n",
       "                                        max_features='sqrt', n_estimators=200,\n",
       "                                        n_jobs=-1))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run CrossValidation and print out the chosen set of hyperparameters that maximized Balanced Accuracy Score\n",
    "best_model = clf_algos_rand.fit(X_train, y_train.ravel())\n",
    "best_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Hyperparameters\n",
    "I ran the RandomCV a number of times to verify (due to the random nature). RandomForestClassifier with class_weight='balanced' was by far the most significant contributor to the higher balanced accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipe = Pipeline([(\n",
    "                       'clf', RandomForestClassifier(class_weight='balanced',\n",
    "                                                     max_depth=10,\n",
    "                                                     max_features='sqrt',\n",
    "                                                     bootstrap=False,\n",
    "                                                     n_estimators = 150,\n",
    "                                                     n_jobs=-1,\n",
    "                                                     criterion='entropy')\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final pipeline (for train against validation)\n",
    "pipe = Pipeline([\n",
    "                 ('prep' , preprocessing),\n",
    "                 ('rf', clf_pipe )\n",
    "                ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model, get Balanced Accuracy Score\n",
    "Getting the new Balanced Accuracy score to compare the improvement over the original LogisticRegression model.  Note that the original score was around 0.66, while the below usually yields around 0.90\n",
    "\n",
    "Also note that this is fit on the train set, and evaluated against the validation set.  Our final model will be fit on train + validation, and evaluated against the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9001436450224278"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train.ravel())\n",
    "y_pred = pipe.predict(X_validate)\n",
    "bal = (balanced_accuracy_score(y_validate.ravel(), y_pred))\n",
    "bal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit against test set for final evaluation\n",
    "For our final model, we want to combine train and validation to train our model on, as this will give the model the maximum amount of data to train on.  We then compare to the test set, which must only be opened once.\n",
    "\n",
    "As we can see, the balanced accuracy score on the test set is also roughly 0.90."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9000255754475703"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final end-to-end pipeline (for train+valid against test set)\n",
    "pipe = Pipeline([\n",
    "                 ('prep' , preprocessing),\n",
    "                 ('rf', clf_pipe )\n",
    "                ])\n",
    "\n",
    "\n",
    "# X_train_pre and y_train_pre are the train sets before being split into train and validation.\n",
    "# i.e. they are already the combined sets of their respective train and validation split.\n",
    "pipe.fit(X_train_pre, y_train_pre.ravel())\n",
    "y_pred = pipe.predict(X_test)\n",
    "bal = (balanced_accuracy_score(y_test.ravel(), y_pred))\n",
    "bal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Through use of Random Forest Classifiers and automated hyperparameter search, our testing balanced accuracy comes out to roughly 90%, a 24% improvement of our initial Logistic Regression model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
